---
bibliography: CHASEref.bib
csl: harvard-cite-them-right.csl
title: CHASE's Group Project
execute:
  echo: false
  freeze: true
format:
  html:
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Liberation Serif
    sansfont: Liberation Sans
    monofont: Liberation Mono
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.4
  kernelspec:
    display_name: Python (base)
    language: python
    name: base
---

```{python}
## Declaration of Authorship {.unnumbered .unlisted}

We, CHASE, pledge our honour that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.

Date: 16 December 2024

Student Numbers: 
24223425
24097459
24081453
24217725
22218425

## Brief Group Reflection

| What Went Well | What Was Challenging |
| -------------- | -------------------- |
| A              | B                    |
| C              | D                    |

## Priorities for Feedback

Are there any areas on which you would appreciate more detailed feedback if we're able to offer it?

{{< pagebreak >}}
```

```{python}
#| echo: false
# importing packages
import os
import pandas as pd
import geopandas as gpd
import matplotlib.patches as mpatches
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from esda import Moran, Moran_Local
from libpysal.weights import Queen
from matplotlib.colors import ListedColormap
from matplotlib.offsetbox import AnchoredText
from matplotlib_scalebar.scalebar import ScaleBar
from scipy import stats
from shapely.geometry import Point
from sklearn.metrics import confusion_matrix
from sklearn.tree import DecisionTreeClassifier, plot_tree
import zipfile
import requests
import importlib.util
import subprocess
import sys
```

```{python}
repo_url = "https://github.com/chrisrichman19/FSDS_CHASE/archive/refs/heads/main.zip"

# Local destination for the ZIP file
zip_path = "repo.zip"

# Download the ZIP file
response = requests.get(repo_url)
with open(zip_path, "wb") as file:
    file.write(response.content)

# Extract the ZIP file
with zipfile.ZipFile(zip_path, "r") as zip_ref:
    zip_ref.extractall(".")

# Remove the ZIP file
os.remove(zip_path)
```

```{python}
def install_and_import(package):
    try:
        # Check if the package is already installed
        if importlib.util.find_spec(package) is None:
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])
    except Exception as e:
        print(f"An error occurred while checking or installing {package}: {e}")

# Link incase this does not work: https://drive.google.com/uc?id=1Abf3I-ez_imd3hzAAmPu8UGcU8dH9lWK

# Google Drive file URL and local file destination:

install_and_import("gdown")
import gdown

file_url = "https://drive.google.com/uc?id=1Abf3I-ez_imd3hzAAmPu8UGcU8dH9lWK"
destination = "London_Points_Of_Interest.gpkg"

# Check if the file already exists
if not os.path.exists(destination):
    gdown.download(file_url, destination, quiet=False)
```

```{python}
# Dowload biobib

# Download harvard-cite-them-right.csl
```

## 1. Who collected the InsideAirbnb data?


The InsideAirbnb was collected by Murray Cox, Tom Slee, and a team of collaborators working to empower communities through data. Murray Cox is an Australian community artist, activist and photo- journalist who "uses data, media, and technology for social change" (@ia_about). (40)


## 2. Why did they collect the InsideAirbnb data?



Murray Cox and Tom Slee collected the InsideAirbnb data to critically assess the impact of Airbnb on housing markets, to provide unbiased independent publicly available data, and to facilitate the improved understanding of city authorities and regulatory bodies. Motivated by an observation of increasing entire-home listings and "multi-lister" hosts, Cox and Slee aimed to challenge Airbnb’s portrayal as a platform for casual home sharing, revealing that much of its revenue comes from commercial operators who are pushing out local residents by raising house prices (@carville_meet_2019). (84)

## 3. How did they collect it?

Inside Airbnb’s data is collected through web-scraping, using public information from Airbnb's website. There were two main stages of data collection:
Identify listings for chosen set of coordinates.
Collect the following information for each listing: listing type, approximate address, number of reviews and average review score, capacity, numbers of bedrooms and bathrooms, price, and coordinates.
Data is periodically scraped for each location from the Airbnb website (@adamiak_airbnb_2019, @ia_home). ()


## 4. How does the method of collection (Q3) impact the completeness and/or accuracy of the InsideAirbnb data? How well does it represent the process it seeks to study, and what wider issues does this raise?


Inside Airbnb (IA) data is limited as scraping can only take place using publicly available data; that which is allowed in Airbnb’s robots.txt file. Datasets are therefore only an approximation of the Airbnb market and might not be suitable for use by those requiring more detailed understanding of Airbnb’s effect on housing markets eg. policymakers.	

Using IA’s data relies solely on the data of Airbnb which only provides an estimation of the short term rental market. Some listings may be booked directly with hosts to avoid Airbnb’s additional charges, and therefore might appear unavailable on Airbnb, but are actually booked; distorting the true effect of Airbnb on the housing market (@prentice_2023).
    
As acknowledged by IA, listings’ geographic locations are not available, meaning that analysis must be aggregated to a higher level. This limits the ability to conduct detailed analysis of Airbnb’s effect on the housing market, as would be the case when conducting price analysis using distance to points of interest as explanatory variables (@todd_2022).

IA programmer (@tslee_airbnbdata) states that ‘no guarantees are made about the quality of data obtained using this script, statistically or about an individual page,’ encouraging researchers to check validity on their own. Furthermore, the script was last updated in 2019 (@tslee_airbnbdata), potentially resulting in inaccurate listing counts following changes to Airbnb’s data structure, reducing the useability of the data to assess housing market impacts.

The IA data is therefore not ‘raw’, it is “verified, cleansed, analyzed and aggregated” @ia_home, meaning the data is biased. Cleaning and aggregation removes detail  and certain perspectives from the data. This erasure is especially significant when those working, processing and analysing the data are not aware of the context in which it was created (@dignazio_5_2020).

Completeness and accuracy challenges raise the question of whether researchers should rely solely on data collected by one organisation, from one website, to analyse the impact of an industry. By focusing solely on the data provided by IA researchers will come to biased and partial conclusions that are influenced by the views of IA’s creators. (345)


## 5. What ethical considerations does the use of the InsideAirbnb data raise? 


Terms of Service outline the contract through which users and Airbnb interact, specifying users must not scrape or use scrapers or crawlers to access or collect data (@airbnb_terms).

From the perspective of an Airbnb user, having agreed to the Terms of Service, they expect that their data is protected. However, as InsideAirbnb demonstrates, these agreements do not guarantee that data will not be collected. Instead, Airbnb are relying on the conscience of the programmers to adhere to the Terms of Service and robot.txt files to guarantee privacy to their users. 

The Terms of Service is contradictory to the robot.txt file- the former specifies not scraping data at all, the latter could be considered as permission to scrape certain data by explicitly prohibiting access to other data.

The robot.txt file is however non-binding, so relies on the programmer’s adherence, which could cause harm to Airbnb’s users and customers if not followed.

InsideAirbnb data is collected through web scraping, in contradiction to the Terms of Service, so despite the efforts taken by IA to anonymise the data, Airbnb users have not given their informed consent for any data to be collected in this way, nor have they consented to their data being used by third parties in this manner, raising ethical considerations over the use of data provided by IA (@krotov_2020).

Another ethical issue is the accuracy of the data being provided. InsideAirbnb state that accuracy of the information compiled from the Airbnb site is not the responsibility of Inside Airbnb. Due care has been taken with any processing and analysis. - (@ia_home).

As @mason_1986 explains, depending on how it is used, the data’s accuracy can raise ethical concerns. InsideAirbnb’s data is “used regularly over the last year by city analysts, journalists, academics and hospitality analysts” @cox_how_airbnb. The use of such data by city analysts will impact the lives of those that fall under the jurisdiction of the city authority, raising the ethical consideration of the data’s accuracy due to the harm that may be caused from the use of Inside Airbnb’s potentially misrepresentative data. This is of further concern when those analysing the data do not fully understand the process through which it has been collected and the limits this brings, causing potentially significant economic and societal impacts if InsideAirbnb’s data is not used mindfully.  

 @mason_1986 argues that accessibility is also an ethical concern when it comes to using data. Not everyone has the technologies required to access the data, and even fewer have the intellectual skills to interpret and process the InsideAirbnb data. @dignazio_5_2020 explain that differential power has a silencing effect and that quantitative data can leave people out. Whilst InsideAirbnb’s mission is to “work towards a vision where communities are empowered with data and information” (@ia_home), its capability to do so is limited if the communities it seeks to empower lack the means to make use of the available data. The lack in transfer of data science skills and knowledge to the communities that InsideAirbnb seek to represent, and subsequent reliance on external researchers, mean that the imbalances in education and power will not be sufficiently addressed since the communities’ reality will only ever be told through the partial perspective of said researchers. (540)

## 6. With reference to the InsideAirbnb data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and the types of properties that they list suggest about the nature of Airbnb lettings in London?

```{python}
# Reading Airbnb data:

airbnb = pd.read_csv("FSDS_CHASE-main/data/20240614-London-listings.csv.gz")

airbnb = gpd.GeoDataFrame(
    airbnb,
    geometry=gpd.points_from_xy(airbnb.longitude, airbnb.latitude),
    crs="EPSG:4326",
).to_crs(epsg=27700)
```

```{python}
# Ward geometry-data:

wards = gpd.read_file("FSDS_CHASE-main/data/London-wards-2011/London_Ward_CityMerged.shp").to_crs(
    epsg=27700
)[["GSS_CODE", "HECTARES", "geometry"]]
boroughs = gpd.read_file(
    "FSDS_CHASE-main/data/statistical-gis-boundaries-london/ESRI/London_Borough_Excluding_MHW.shp"
).to_crs(epsg=27700)
```

```{python}
# Filtering Airbnb data:

from datetime import datetime, timedelta

# Filter min. nights
filtered_airbnb = airbnb[airbnb.minimum_nights <= 30]

## To determine recently active sites

# Convert 'last_review' to datetime format
filtered_airbnb.loc[:, "last_review"] = pd.to_datetime(
    filtered_airbnb["last_review"], errors="coerce"
)

# Set the reference date (for reproducibility)
reference_date = datetime(2024, 11, 28)

# Calculate the threshold date (6 months before the reference date)
six_months_ago = reference_date - timedelta(days=6 * 30)

# Filter the data
filtered_airbnb = filtered_airbnb[filtered_airbnb["last_review"] >= six_months_ago]

filtered_airbnb = filtered_airbnb[filtered_airbnb["availability_365"] >= 90]
```

### Distribution of Short-Term Listings per Borough with Average Price 

```{python}
# Creating a distribution of airbnb map with average price as colour base
# Performing a spatial join to assign each Airbnb listing to a borough
airbnb_in_boroughs = gpd.sjoin(filtered_airbnb, boroughs, how="inner", predicate="within")

# Calculating the average price of Airbnb listings in each borough
borough_prices = (
    airbnb_in_boroughs.groupby("NAME")["price"].mean().reset_index(name="Average_Price")
)

# Rounding the average prices and merging back with boroughs
borough_prices["Average_Price"] = borough_prices["Average_Price"].round(0)
boroughs = boroughs.merge(borough_prices, on="NAME", how="left")

# Filling in NaN values for boroughs without listings
boroughs["Average_Price"] = boroughs["Average_Price"].fillna(0)

# Plotting the choropleth map with Airbnb points
fig, ax = plt.subplots(figsize=(12, 12))

# Plotting the boroughs and shading based on Average Price
choropleth = boroughs.plot(
    column="Average_Price",
    cmap="YlOrRd",  
    legend=True,
    edgecolor="black",
    linewidth=0.5,
    ax=ax,
)

# Customizing the colorbar for Average Price
colorbar = choropleth.get_figure().get_axes()[1]
colorbar.set_title("Avg Price\n(£)", fontsize=12)
colorbar.text(1.3, 1.01, "High", fontsize=10, transform=ax.transAxes, ha="center")
colorbar.text(1.3, -0.01, "Low", fontsize=10, transform=ax.transAxes, ha="center")

# Plotting Airbnb points on top
filtered_airbnb.plot(
    ax=ax, color="blue", markersize=5, alpha=0.5, label="Airbnb Listings"
)

# Adding a north arrow to the top-left corner
north_arrow = AnchoredText(
    "N ↑", loc="upper left", prop=dict(size=15), frameon=False, borderpad=1
)
ax.add_artist(north_arrow)

# Customizing the legend and placing it on the right
airbnb_patch = mpatches.Patch(color="blue", label="Airbnb Listings")
legend = ax.legend(
    handles=[airbnb_patch],
    title="Legend",
    fontsize=12,
    loc="upper right",
    title_fontsize=13,
)

# Adding title and labels
plt.title("Average Price of Short-Term Airbnb Listings in London Boroughs (2024)", fontsize=16)
ax.set_xlabel("Longitude", fontsize=12)
ax.set_ylabel("Latitude", fontsize=12)

# Final adjustments and display the map
plt.tight_layout()
plt.show()
```

Airbnb’s short-term listings are clustered in boroughs surrounding the City of London. 14.4% of listings occur in Westminster, followed by 8.1% in Kensington and Chelsea, then 7.5% in Camden.

Westminster, Kensington and Chelsea, and the City of London exhibit the highest average prices, while Bexley and Harrow have the lowest, reflecting significant variation in short-term rental costs across the city.

```{python}
# London Ward profiles:

ward_profiles = pd.read_csv("FSDS_CHASE-main/data/ward-profiles.csv", encoding="latin1")

# Including only selected variables
ward_profiles = ward_profiles[
    [
        "Ward name",
        "New code",
        "Average Public Transport Accessibility score - 2014",
        "Median House Price (£) - 2014",
        "Median Household income estimate (2012/13)",
        "(ID2010) - Rank of average score (within London) - 2010",
        "(ID2010) % of LSOAs in worst 50% nationally - 2010",
        "Average GCSE capped point scores - 2014",
        "% BAME - 2011",
        "Number of Household spaces - 2011",
    ]
]
```

```{python}
# Listing all Airbnb counts and calculating percentages of airbnb listings in each borough. 
# Performing spatial join to assign each Airbnb to a borough
airbnb_in_boroughs = gpd.sjoin(
    filtered_airbnb, boroughs, how="inner", predicate="within"
)

# Count the number of Airbnb listings in each borough
borough_counts = (
    airbnb_in_boroughs.groupby("NAME").size().reset_index(name="Airbnb_Counts")
)

# Fill NaN values with 0 in case of missing boroughs
borough_counts = borough_counts.fillna(0)

# Calculate the total number of Airbnb listings
total_listings = borough_counts["Airbnb_Counts"].sum()

# Calculate the percentage of listings in each borough
borough_counts["Percentage"] = (borough_counts["Airbnb_Counts"] / total_listings) * 100

# Sort the counts by the percentage of listings
borough_counts = borough_counts.sort_values(by="Percentage", ascending=False)

# Print borough names with airbnb percentages - Used the results in the text (do not show in actual output)
#print("Boroughs with Airbnb Percentages:")
#print(borough_counts[["NAME", "Airbnb_Counts", "Percentage"]])

# Save the result to a CSV file (optional)
borough_counts.to_csv("airbnb_percentage_by_borough.csv", index=False)
```

### Properties Available for over 90+ Nights

A concern that Airbnb imposes in London is ‘commercialisation’. The Greater London Authority (GLA) states that “it creates a risk of residential properties being used as letting businesses without the required planning permission and protections for neighbours” (@balogun_2024, p.26). To avoid this issue, homeowners are required to obtain planning permission if they intend to use residential properties for short-term accommodation exceeding 90 nights.
In the current 2024 analysis, we found that there is a total of 6254 listings available for over 90+ nights. Westminster has 15.7% of those listings, Kensington and Chelsea at 9%, and Tower Hamlets at 7.5%.

```{python}
# Filtering listings for 90+ nights of availability
filtered_airbnb = filtered_airbnb[filtered_airbnb["maximum_nights"] >= 90]

# Performing a spatial join to assign each Airbnb to a borough
airbnb_in_boroughs = gpd.sjoin(
    filtered_airbnb, boroughs, how="inner", predicate="within"
)

# Counting the number of Airbnb listings in each borough
borough_counts = (
    airbnb_in_boroughs.groupby("NAME").size().reset_index(name="Airbnb_Counts")
)

# Calculating the percentage of listings for each borough
total_listings = borough_counts["Airbnb_Counts"].sum()
borough_counts["Percentage"] = (borough_counts["Airbnb_Counts"] / total_listings) * 100

# Displaying results - Used the results in the text (do not show in actual output)
#print("Airbnb Listings Available for 90+ nights by Borough (With Percentage):")
#print(borough_counts)

# Displaying the total number of listings - Used the results in the text (do not show in actual output)
#print(f"\nTotal Airbnb Listings Available for 90+ nights: {total_listings}")
```

```{python}
# Filtering listings for 90+ nights of availability
filtered_airbnb = filtered_airbnb[filtered_airbnb["availability_365"] >= 90]

# Total number of listings
total_properties = len(filtered_airbnb)
print(f"Total Airbnb Listings Available for 90+ Nights: {total_properties}")

# Grouping by room type to count properties rented for 90+ nights
room_type_counts = filtered_airbnb["room_type"].value_counts().reset_index()
room_type_counts.columns = ["Room Type", "Count"]

# Printing the room type counts - Used the results in the text (do not show in actual output)
#print("\nRoom Types Available for 90+ Nights:")
#print(room_type_counts)

# Plotting the room types as a bar graph
plt.figure(figsize=(8, 6))
plt.bar(
    room_type_counts["Room Type"],
    room_type_counts["Count"],
    color="orange",
    edgecolor="black",
)

# Adding titles and labels 
plt.title("Room Types Rented for 90+ Nights", fontsize=16)
plt.xlabel("Room Type", fontsize=12)
plt.ylabel("Number of Properties", fontsize=12)

# Adding data labels on top of each bar
for i, value in enumerate(room_type_counts["Count"]):
    plt.text(i, value + 5, str(value), ha="center", va="bottom", fontsize=10)

# Displaying the graph
plt.tight_layout()
plt.show()
```

The most common ‘room types’ available for over 90+ nights are entire homes and apartments.

### Hosts with Multiple Listings

GLA have discovered that hosts with multiple listings on Airbnb are more likely to be using the platform for commercial purposes (@balogun_2024, 2024, p.25). The total number of hosts with two or more listings is 6253 with the average number of listings per host being 7.06. The maximum number of listings is 1253 that belong to Sykes Holiday Cottages. This shows that the Airbnb market in London is commercialised and does not adhere to the GLA 90-night policy limit. 

```{python}
# Filter hosts with total listings greater than 2
hosts_with_2plus_listings = airbnb[airbnb["host_listings_count"] >= 2]

# Group by host_name and aggregate total listings
hosts_summary = (
    hosts_with_2plus_listings.groupby("host_name")
    .agg(total_listings=("host_listings_count", "first"))
    .reset_index()
)

# Calculate required metrics
total_hosts = hosts_summary.shape[0]
average_listings = hosts_summary["total_listings"].mean()
max_listings = hosts_summary["total_listings"].max()
min_listings = hosts_summary["total_listings"].min()

# Find the host(s) with the most listings
host_with_max_listings = hosts_summary[hosts_summary["total_listings"] == max_listings]

# Print the results
#print(f"Total number of hosts with more than 2 listings: {total_hosts}")
#print(f"Average number of listings per host: {average_listings:.2f}")
#print(f"Maximum number of listings: {max_listings}")
#print(f"Minimum number of listings (2 or more): {min_listings}")

#print("\nHost(s) with the most listings:")
#print(host_with_max_listings)
```

## 7. Drawing on your previous answers, and supporting your response with evidence (*e.g.* figures, maps, EDA/ESDA, and simple statistical analysis/models drawing on experience from, e.g., CASA0007), how *could* the InsideAirbnb data set be used to inform the regulation of Short-Term Lets (STL) in London? 

Insights from the previous section highlight the commercialized reality of Airbnb in London despite official regulations. Failure to limit Airbnb has the potential to exacerbate the affordable housing crisis in London by reducing the availability of long-term rentals. The impact of Airbnb is of particular concern with regard to deprived households, whereby increased housing costs can contribute to displacement, making it more difficult for these residents to remain in their communities.

Our analysis aims to answer the following questions: 

- What wards are “at risk” of becoming an Airbnb hotspot?

- Of these wards, which are also the most vulnerable to the negative social impacts of Airbnb?

We filtered the Airbnb dataset to include only short-term rentals (≤30 nights), recently active listings (reviews within six months), properties available ≥90 days annually, and "Entire home/apt" listings, as these have the greatest impact on housing and neighbourhood dynamics.

```{python}
# Filtering Data again (Q6 has other filters in place):

from datetime import datetime, timedelta

# Filter min. nights
filtered_airbnb = airbnb[airbnb.minimum_nights <= 30]

# To determine recently active sites
# Convert 'last_review' to datetime format
filtered_airbnb.loc[:, "last_review"] = pd.to_datetime(
    filtered_airbnb["last_review"], errors="coerce"
)

# Set the reference date (for reproducibility)
reference_date = datetime(2024, 11, 28)

# Calculate the threshold date (6 months before the reference date)
six_months_ago = reference_date - timedelta(days=6 * 30)

# Filter the data
filtered_airbnb = filtered_airbnb[filtered_airbnb["last_review"] >= six_months_ago]

filtered_airbnb = filtered_airbnb[filtered_airbnb["availability_365"] >= 90]

filtered_airbnb = filtered_airbnb[filtered_airbnb["room_type"] == "Entire home/apt"]
```

```{python}
#Reading in Point of Interest data:

gdf = gpd.read_file("London_Points_Of_Interest.gpkg")

# Selecting pubs, restaurants, cafes
desired_classes = [
    "Pubs, Bars and Inns",
    "Cafes, Snack Bars and Tea Rooms",
    "Restaurants",
]
poi_data = gdf[gdf["classname"].isin(desired_classes)]
```

```{python}
# Merging files together:

ward_airbnb = wards.merge(
    ward_profiles, left_on="GSS_CODE", right_on="New code", how="left"
)

ward_airbnb = gpd.GeoDataFrame(ward_airbnb, crs="EPSG:27700")

ward_airbnb["Borough"] = ward_airbnb["Ward name"].str.split(" - ").str[0]

# Creating poi_density variable
ward_airbnb["n_poi"] = ward_airbnb.geometry.apply(
    lambda geom: sum(poi_data.geometry.intersects(geom))
)
ward_airbnb["poi_density"] = ward_airbnb["n_poi"] / ward_airbnb["HECTARES"]

# Intersecting airbnb points and ward data
ward_airbnb["n_airbnb"] = ward_airbnb.geometry.apply(
    lambda geom: sum(filtered_airbnb.geometry.intersects(geom))
)

# Calculate Airbnb density
ward_airbnb["airbnb_density"] = ward_airbnb["n_airbnb"] / ward_airbnb["HECTARES"]

# Creating airbnb per 1000 households variables
ward_airbnb["airbnb_per_1000_household"] = (
    ward_airbnb["n_airbnb"] / ward_airbnb["Number of Household spaces - 2011"]
) * 1000
```

### Analysis 
#### Decision Tree Model 

We chose a decision tree methodology for this analysis due to its high interpretability, making it easy for policymakers to understand how each ward was classified as “at-risk” or “too late”. Each classification decision the model makes can be easily traced in a simple, visual format.

The decision tree predicts whether a ward is likely to have high Airbnb density using ward-level characteristics of public transport accessibility, house prices, and point of interest density, which we then use to categorise each London ward into one of three groups:

**Too Late**: Wards already heavily impacted by Airbnb, that are in the top 5% of airbnb's per 1000 households.

**At Risk**: Wards predicted to have high Airbnb density but do not meet the threshold.

**Neither**: Wards that don't fall into either category.

We selected these variables as research has shown that higher-income neighborhoods, better transit access, and proximity to attractions significantly influence Airbnb activity (@jiao_empirical_2020).

```{python}
# Calculate the 95th quantile for Airbnb density
quantile = np.quantile(ward_airbnb["airbnb_per_1000_household"], 0.95)
print(f"AirBnb Density 'Too-Late' Threshold: {quantile:.3} per 1000 households\n")


# Create a high Airbnb density indicator
ward_airbnb["high_airbnb_dens"] = ward_airbnb["airbnb_per_1000_household"] >= quantile

print(
    f"Average Ward AirBnb Density: {ward_airbnb['airbnb_per_1000_household'].mean():.3} per 1000 households "
)
print(
    f"Average Ward House Price: £{ward_airbnb['Median House Price (£) - 2014'].mean():,.0f} "
)
print(
    f"Average Ward Point of Interest Density: {ward_airbnb['poi_density'].mean():.3} "
)
print(
    f"Average Ward Public Transport Accessibility Score: {ward_airbnb['Average Public Transport Accessibility score - 2014'].mean():.3}"
)
```

```{python}
# Prepare data for the decision tree
X = ward_airbnb[
    [
        "poi_density",
        "Average Public Transport Accessibility score - 2014",
        "Median House Price (£) - 2014",
        "Median Household income estimate (2012/13)",
    ]
]  # dependent variables
y = ward_airbnb["high_airbnb_dens"]

# Train a decision tree
tree = DecisionTreeClassifier(
    max_depth=2, random_state=42
)  # limited depth means model remains interpretable without overfitting, focusing on the most critical features.
tree.fit(X, y)

# Plot the decision tree
plt.figure(figsize=(12, 8))
plot_tree(
    tree,
    feature_names=X.columns,
    class_names=["Low Density", "High Density"],
    filled=True,
    fontsize=10,
)
plt.show()
```

#### Interpreting the Decision Tree

The decision tree model identifies two scenarios where a ward is predicted to have a high Airbnb density:

- Transport Accessibility score of more than 6.75 and a Median House Price of more than £477,000

**or**

- Transport Accessibility score of less than or equal to 6.75 and a point of interest density of more than 1.059 per hectare.

The results suggest that high Airbnb density is linked to well-connected areas with above-average housing prices, though less connected areas can also attract Airbnb activity if they offer a high concentration of attractions and amenities.

There are 9 wards that meet either of these conditions but do not exceed the ‘too late’ Airbnb density threshold, so we have categorised them as ‘at-risk’.

```{python}
# Add predictions to the GeoDataFrame
ward_airbnb["prediction"] = tree.predict_proba(X)[:, 1] > 0.5

# Generate the at-risk and too-late categories
at_risk = ward_airbnb[
    (ward_airbnb["high_airbnb_dens"] == False) & (ward_airbnb["prediction"] == True)
]
too_late = ward_airbnb[ward_airbnb["high_airbnb_dens"] == True]

ward_airbnb["status"] = "neither"
ward_airbnb.loc[ward_airbnb["Ward name"].isin(too_late["Ward name"]), "status"] = (
    "too_late"
)
ward_airbnb.loc[ward_airbnb["Ward name"].isin(at_risk["Ward name"]), "status"] = (
    "at_risk"
)
```

```{python}
# Visualise at risk wards with a choropleth map
fig, ax = plt.subplots(1, 1, figsize=(10, 8))
ward_airbnb.plot(
    column="status",
    legend=True,
    cmap="coolwarm",
    ax=ax,
    legend_kwds={"bbox_to_anchor": (1, 1)},
)
plt.title("Ward Status by Airbnb Density")

# Add scale bar
scalebar = ScaleBar(
    1, location="lower left"
)  # 1 is the length of one pixel, adjust based on map scale
ax.add_artist(scalebar)

# Add North Arrow
x, y, arrow_length = 0.05, 0.98, 0.1
ax.annotate(
    "N",
    xy=(x, y),
    xytext=(x, y - arrow_length),
    arrowprops=dict(facecolor="black", width=5, headwidth=15),
    ha="center",
    va="center",
    fontsize=20,
    xycoords=ax.transAxes,
)

boroughs.plot(edgecolor="lightgrey", linewidth=1, ax=ax, facecolor="none")
plt.show()
```

### "At-Risk" Wards and Deprivation

To assess potential social impact of Airbnb in London, we analyzed the relationship between our Airbnb decision tree classifications and deprivation rank in wards. Deprivation rank is a relative measure comparing the level of deprivation across London wards. The lower the deprivation rank, the greater the deprivation score in the ward.

#### EDA

```{python}
# List of variables to analyze
variables = ["(ID2010) - Rank of average score (within London) - 2010"]

# Custom order for the categories
category_order = ["at_risk", "too_late", "neither"]

# Group the data by 'status' and calculate medians
averages = ward_airbnb.groupby("status")[variables].mean()

# Reindex to ensure the correct order of categories
averages = averages.reindex(category_order)

print(
    f"Mean Deprivation Rank for 'at-risk' wards: {averages.loc['at_risk'].iloc[0]:.0f}"
)
print(
    f"Mean Deprivation Rank for 'too-late' wards: {averages.loc['too_late'].iloc[0]:.0f}"
)
print(
    f"Mean Deprivation Rank for 'neither' wards: {averages.loc['neither'].iloc[0]:.0f}"
)
```

We begin with an exploratory data analysis of deprivation and ward classifications. The results shows the average deprivation rank between classification types. We see that the mean deprivation of “at-risk” wards is lower than “too-late” wards. This suggests that “at-risk” wards are not only desirable to Airbnb letters based on our classification, but are also particularly vulnerable to the negative impacts of Airbnb due to these areas being more deprived on average.

#### Spatial Autocorrelation Analysis

The question of remains where overlaps between “at-risk” wards and high deprivation occur. Spatial autocorrelation analysis of deprivation in London wards was employed to assess the spatial distribution of deprived wards and examine which “at-risk” wards lie within high deprivation clusters. Cluster analysis improves the generalizability of the study by identifying patterns of deprivation beyond individual wards with arbitrary boundaries.

Moran's I:

```{python}
# Create a spatial weights matrix based on Queen contiguity
w = Queen.from_dataframe(ward_airbnb, use_index=False)

# Ensure the weights are row-standardized
w.transform = "r"

# Use the variable '(ID2010) - Rank of average score (within London) - 2010' for the Moran test
y = ward_airbnb["(ID2010) - Rank of average score (within London) - 2010"].values

# Calculate Global Moran's I
moran = Moran(y, w)

# Moran's I statistic
moran_I = moran.I
print(f"Global Moran's I: {moran_I}")

# p-value for the test
moran_p_value = moran.p_sim
print(f"p-value: {moran_p_value}")
```

A Moran’s I test is conducted to establish that deprivation is not randomly distributed across wards in London. A Global Moran’s I statistic of 0.689 with a p-value less than 0.05 indicates there is statistically significant clustering of deprivation in London wards.

LISA:

```{python}
# Ensure the weights are row-standardized
w.transform = "r"

# Use the vulnerability variable for LISA analysis
y = ward_airbnb["(ID2010) - Rank of average score (within London) - 2010"].values

# Calculate Local Moran's I
lisa = Moran_Local(y, w)

# Add the LISA statistics to the GeoDataFrame
ward_airbnb["moran_local"] = lisa.Is
ward_airbnb["p_value"] = lisa.p_sim
ward_airbnb["quadrant"] = lisa.q

# Identify significant clusters at 0.05 level
ward_airbnb["significant"] = ward_airbnb["p_value"] < 0.05

# Define custom colormap for the LISA quadrants
custom_cmap = ListedColormap(
    ["navy", "skyblue", "darkred", "pink"]
)  # Soft dark red, soft light blue, soft dark blue, soft light red

# Plot the LISA results
fig, ax = plt.subplots(1, 1, figsize=(10, 10))

# Use 'quadrant' for the categorical color mapping
ward_airbnb.plot(
    column="quadrant",  # Use 'quadrant' for discrete categories
    cmap=custom_cmap,  # Apply the custom colormap
    ax=ax,
    legend=False,  # Disable the color bar
)

# Outline the wards with 'at_risk' status
at_risk_wards = ward_airbnb[ward_airbnb["status"] == "at_risk"]
at_risk_wards.plot(
    edgecolor="yellow",  # Set black boundary for at-risk wards
    linewidth=2,  # Thicker line width for visibility
    ax=ax,
    facecolor="none",  # No fill color, just outlines
)

# Outline the wards with 'at_risk' status
boroughs.plot(
    edgecolor="lightgrey",  # Set black boundary for at-risk wards
    linewidth=1,  # Thicker line width for visibility
    ax=ax,
    facecolor="none",  # No fill color, just outlines
)

# Manually create legend with cluster labels
legend_labels = ["High-High", "Low-High", "Low-Low", "High-Low"]
colors = custom_cmap(range(4))  # Get colors from the custom colormap

# Add the custom legend to the plot
legend_elements = [
    mpatches.Patch(color=colors[i], label=legend_labels[i]) for i in range(4)
]
ax.legend(handles=legend_elements, title="LISA Cluster")

# Add scale bar
scalebar = ScaleBar(
    1, location="lower left"
)  # 1 is the length of one pixel, adjust based on map scale
ax.add_artist(scalebar)

# Add North Arrow
x, y, arrow_length = 0.05, 0.98, 0.1
ax.annotate(
    "N",
    xy=(x, y),
    xytext=(x, y - arrow_length),
    arrowprops=dict(facecolor="black", width=5, headwidth=15),
    ha="center",
    va="center",
    fontsize=20,
    xycoords=ax.transAxes,
)

plt.title('LISA Analysis of Deprivation Rank, Highlighting "At-Risk" Wards')
plt.show()
```

Local indicators of spatial autocorrelation (LISA) statistics allow us to visualize the clustering of deprivation in London. Outlining “at-risk” wards in yellow, we see an overlap of deprivation clustering and “at-risk” classification for three wards in Islington, one in Hackney, one in Camden, and one in Hammersmith and Fulham.

## Conclusion

The study concludes there are 9 wards “at-risk” of becoming heavily saturated by Airbnb. Furthermore, six of these wards are located within clusters of high deprivation. City policy should focus on better regulating and limiting Airbnbs in the the boroughs of Islington, Hackney, and Hammersmith and Fulham to mitigate the negative impacts of Airbnb on vulnerable populations.

## Limitations

Significant limitations to this study remain. We chose to use data from the 2014 London Ward Atlas in order to incorporate public transit accessibility into our analysis. However, this approach means that all other variables used from this dataset (i.e. house price and deprivation rank) are equally 10+ years outdated. Results from this study should be verified when updated accessibility scores for current wards become publicly available.  Additionally, a purely quantitate analysis cannot comprehensively capture lived experience and local context. Qualitative work in our specified wards would be valuable for gaining better insights to the impact of Airbnb in these areas.

## References
